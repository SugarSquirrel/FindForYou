"""
DINOv2 ç‰¹å¾µæå–å™¨æ¨¡çµ„
ä½¿ç”¨ Meta çš„ DINOv2 æ¨¡å‹æå–è¦–è¦ºç‰¹å¾µ
"""

import torch
import torch.nn.functional as F
from torchvision import transforms
from PIL import Image
import numpy as np
from typing import Union, List
import os


class FeatureExtractor:
    """DINOv2 ç‰¹å¾µæå–å™¨"""
    
    def __init__(self, model_name: str = "dinov2_vits14", device: str = None):
        """
        åˆå§‹åŒ– DINOv2 æ¨¡å‹
        
        Args:
            model_name: æ¨¡å‹åç¨±ï¼Œé¸é …ï¼š
                - dinov2_vits14 (384 ç¶­, æœ€è¼•é‡)
                - dinov2_vitb14 (768 ç¶­)
                - dinov2_vitl14 (1024 ç¶­)
                - dinov2_vitg14 (1536 ç¶­, æœ€å¤§)
            device: é‹ç®—è£ç½® ('cuda' æˆ– 'cpu')
        """
        self.model_name = model_name
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.transform = None
        self.is_ready = False
        
        self._init_model()
    
    def _init_model(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        try:
            print(f"ğŸ”„ è¼‰å…¥ DINOv2 æ¨¡å‹: {self.model_name}...")
            
            # å¾ torch hub è¼‰å…¥ DINOv2
            self.model = torch.hub.load(
                'facebookresearch/dinov2', 
                self.model_name,
                pretrained=True
            )
            self.model = self.model.to(self.device)
            self.model.eval()
            
            # è¨­å®šåœ–ç‰‡è½‰æ›
            self.transform = transforms.Compose([
                transforms.Resize(256),
                transforms.CenterCrop(224),
                transforms.ToTensor(),
                transforms.Normalize(
                    mean=[0.485, 0.456, 0.406],
                    std=[0.229, 0.224, 0.225]
                )
            ])
            
            self.is_ready = True
            print(f"âœ… DINOv2 æ¨¡å‹å·²è¼‰å…¥: {self.model_name} (è£ç½®: {self.device})")
            
        except Exception as e:
            print(f"âŒ DINOv2 æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            self.is_ready = False
    
    def extract_features(
        self, 
        image: Union[np.ndarray, Image.Image, str]
    ) -> np.ndarray:
        """
        æå–åœ–ç‰‡çš„ç‰¹å¾µå‘é‡
        
        Args:
            image: è¼¸å…¥åœ–ç‰‡ (numpy array, PIL Image, æˆ–æª”æ¡ˆè·¯å¾‘)
            
        Returns:
            ç‰¹å¾µå‘é‡ (numpy array)
        """
        if not self.is_ready:
            raise RuntimeError("DINOv2 æ¨¡å‹æœªå°±ç·’")
        
        # è½‰æ›ç‚º PIL Image
        if isinstance(image, str):
            pil_image = Image.open(image).convert('RGB')
        elif isinstance(image, np.ndarray):
            # OpenCV BGR to RGB
            if len(image.shape) == 3 and image.shape[2] == 3:
                image = image[:, :, ::-1]
            pil_image = Image.fromarray(image).convert('RGB')
        elif isinstance(image, Image.Image):
            pil_image = image.convert('RGB')
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„åœ–ç‰‡é¡å‹: {type(image)}")
        
        # è½‰æ›ä¸¦æå–ç‰¹å¾µ
        with torch.no_grad():
            img_tensor = self.transform(pil_image).unsqueeze(0).to(self.device)
            features = self.model(img_tensor)
            
            # æ­£è¦åŒ–ç‰¹å¾µ
            features = F.normalize(features, p=2, dim=1)
            
        return features.cpu().numpy().flatten()
    
    def extract_features_batch(
        self, 
        images: List[Union[np.ndarray, Image.Image]]
    ) -> np.ndarray:
        """
        æ‰¹æ¬¡æå–å¤šå¼µåœ–ç‰‡çš„ç‰¹å¾µ
        
        Args:
            images: åœ–ç‰‡åˆ—è¡¨
            
        Returns:
            ç‰¹å¾µçŸ©é™£ (N x feature_dim)
        """
        if not self.is_ready:
            raise RuntimeError("DINOv2 æ¨¡å‹æœªå°±ç·’")
        
        tensors = []
        for img in images:
            if isinstance(img, np.ndarray):
                if len(img.shape) == 3 and img.shape[2] == 3:
                    img = img[:, :, ::-1]
                pil_img = Image.fromarray(img).convert('RGB')
            else:
                pil_img = img.convert('RGB')
            
            tensors.append(self.transform(pil_img))
        
        with torch.no_grad():
            batch = torch.stack(tensors).to(self.device)
            features = self.model(batch)
            features = F.normalize(features, p=2, dim=1)
            
        return features.cpu().numpy()
    
    @staticmethod
    def cosine_similarity(
        embedding1: np.ndarray, 
        embedding2: np.ndarray
    ) -> float:
        """
        è¨ˆç®—å…©å€‹ç‰¹å¾µå‘é‡çš„é¤˜å¼¦ç›¸ä¼¼åº¦
        
        Args:
            embedding1: ç¬¬ä¸€å€‹ç‰¹å¾µå‘é‡
            embedding2: ç¬¬äºŒå€‹ç‰¹å¾µå‘é‡
            
        Returns:
            ç›¸ä¼¼åº¦ (0~1)
        """
        # ç¢ºä¿æ˜¯ 1D å‘é‡
        e1 = embedding1.flatten()
        e2 = embedding2.flatten()
        
        # è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦
        dot_product = np.dot(e1, e2)
        norm1 = np.linalg.norm(e1)
        norm2 = np.linalg.norm(e2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return float(dot_product / (norm1 * norm2))
    
    @staticmethod
    def find_best_match(
        query_embedding: np.ndarray,
        embeddings_list: List[np.ndarray],
        threshold: float = 0.7
    ) -> tuple:
        """
        åœ¨åµŒå…¥åˆ—è¡¨ä¸­æ‰¾åˆ°æœ€ä½³åŒ¹é…
        
        Args:
            query_embedding: æŸ¥è©¢ç‰¹å¾µå‘é‡
            embeddings_list: å€™é¸ç‰¹å¾µå‘é‡åˆ—è¡¨
            threshold: ç›¸ä¼¼åº¦é–¾å€¼
            
        Returns:
            (æœ€ä½³åŒ¹é…ç´¢å¼•, ç›¸ä¼¼åº¦) æˆ– (-1, 0.0) è‹¥ç„¡åŒ¹é…
        """
        if not embeddings_list:
            return (-1, 0.0)
        
        best_idx = -1
        best_sim = 0.0
        
        for idx, emb in enumerate(embeddings_list):
            sim = FeatureExtractor.cosine_similarity(query_embedding, emb)
            if sim > best_sim:
                best_sim = sim
                best_idx = idx
        
        if best_sim >= threshold:
            return (best_idx, best_sim)
        else:
            return (-1, best_sim)
    
    def get_feature_dim(self) -> int:
        """å–å¾—ç‰¹å¾µç¶­åº¦"""
        dims = {
            "dinov2_vits14": 384,
            "dinov2_vitb14": 768,
            "dinov2_vitl14": 1024,
            "dinov2_vitg14": 1536,
        }
        return dims.get(self.model_name, 384)
