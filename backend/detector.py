"""
æ‰¾æ±è¥¿åŠ©æ‰‹ - YOLO12 + DINOv2 ç‰©ä»¶åµæ¸¬å™¨
ç´”æ¨è«–æ¨¡å¼ï¼šæ¥æ”¶åœ–ç‰‡ â†’ YOLO12 åµæ¸¬ â†’ DINOv2 ç‰¹å¾µæ¯”å° â†’ è¿”å›çµæœ
"""

import cv2
import numpy as np
from typing import List, Optional
from dataclasses import dataclass, field
from datetime import datetime

from ultralytics import YOLO

from feature_extractor import FeatureExtractor
from object_registry import ObjectRegistry


@dataclass
class DetectionResult:
    """åµæ¸¬çµæœè³‡æ–™çµæ§‹"""
    object_class: str
    object_class_zh: str
    confidence: float
    bbox: List[float]
    matched_object_id: Optional[str] = None
    matched_object_name: Optional[str] = None
    matched_object_name_zh: Optional[str] = None
    similarity: Optional[float] = None
    surface: Optional[str] = None
    region: Optional[str] = None
    timestamp: int = field(default_factory=lambda: int(datetime.now().timestamp() * 1000))
    
    def to_dict(self):
        return {
            "object_class": self.object_class,
            "object_class_zh": self.object_class_zh,
            "confidence": self.confidence,
            "bbox": self.bbox,
            "matched_object_id": self.matched_object_id,
            "matched_object_name": self.matched_object_name,
            "matched_object_name_zh": self.matched_object_name_zh,
            "similarity": self.similarity,
            "surface": self.surface,
            "region": self.region,
            "timestamp": self.timestamp
        }


# COCO é¡åˆ¥å°ç…§è¡¨
CLASS_NAMES_ZH = {
    "person": "äºº", "bicycle": "è…³è¸è»Š", "car": "æ±½è»Š", "motorcycle": "æ©Ÿè»Š",
    "airplane": "é£›æ©Ÿ", "bus": "å…¬è»Š", "train": "ç«è»Š", "truck": "å¡è»Š",
    "boat": "èˆ¹", "traffic light": "äº¤é€šç‡ˆ", "fire hydrant": "æ¶ˆé˜²æ “",
    "stop sign": "åœæ­¢æ¨™èªŒ", "parking meter": "åœè»Šæ”¶è²»è¡¨", "bench": "é•·å‡³",
    "bird": "é³¥", "cat": "è²“", "dog": "ç‹—", "horse": "é¦¬", "sheep": "ç¾Š",
    "cow": "ç‰›", "elephant": "å¤§è±¡", "bear": "ç†Š", "zebra": "æ–‘é¦¬",
    "giraffe": "é•·é ¸é¹¿", "backpack": "èƒŒåŒ…", "umbrella": "é›¨å‚˜",
    "handbag": "æ‰‹æåŒ…", "tie": "é ˜å¸¶", "suitcase": "è¡Œæç®±",
    "frisbee": "é£›ç›¤", "skis": "æ»‘é›ªæ¿", "snowboard": "æ»‘é›ªæ¿",
    "sports ball": "é‹å‹•çƒ", "kite": "é¢¨ç®", "baseball bat": "æ£’çƒæ£’",
    "baseball glove": "æ£’çƒæ‰‹å¥—", "skateboard": "æ»‘æ¿", "surfboard": "è¡æµªæ¿",
    "tennis racket": "ç¶²çƒæ‹", "bottle": "æ°´ç“¶", "wine glass": "é…’æ¯",
    "cup": "æ¯å­", "fork": "å‰å­", "knife": "åˆ€å­", "spoon": "æ¹¯åŒ™",
    "bowl": "ç¢—", "banana": "é¦™è•‰", "apple": "è˜‹æœ", "sandwich": "ä¸‰æ˜æ²»",
    "orange": "æ©˜å­", "broccoli": "èŠ±æ¤°èœ", "carrot": "èƒ¡è˜¿è””",
    "hot dog": "ç†±ç‹—", "pizza": "æŠ«è–©", "donut": "ç”œç”œåœˆ", "cake": "è›‹ç³•",
    "chair": "æ¤…å­", "couch": "æ²™ç™¼", "potted plant": "ç›†æ ½", "bed": "åºŠ",
    "dining table": "é¤æ¡Œ", "toilet": "é¦¬æ¡¶", "tv": "é›»è¦–",
    "laptop": "ç­†é›»", "mouse": "æ»‘é¼ ", "remote": "é™æ§å™¨",
    "keyboard": "éµç›¤", "cell phone": "æ‰‹æ©Ÿ", "microwave": "å¾®æ³¢çˆ",
    "oven": "çƒ¤ç®±", "toaster": "çƒ¤éºµåŒ…æ©Ÿ", "sink": "æ°´æ§½",
    "refrigerator": "å†°ç®±", "book": "æ›¸", "clock": "æ™‚é˜",
    "vase": "èŠ±ç“¶", "scissors": "å‰ªåˆ€", "teddy bear": "æ³°è¿ªç†Š",
    "hair drier": "å¹é¢¨æ©Ÿ", "toothbrush": "ç‰™åˆ·",
    # Finetune æ¨¡å‹è‡ªè¨‚é¡åˆ¥
    "cell_phone": "æ‰‹æ©Ÿ", "wallet": "éŒ¢åŒ…", "key": "é‘°åŒ™",
    "remote_control": "é™æ§å™¨", "watch": "æ‰‹éŒ¶", "earphone": "è€³æ©Ÿ",
    "cup": "æ¯å­", "bottle": "æ°´ç“¶"
}


class ObjectDetector:
    """YOLO12 + DINOv2 ç‰©ä»¶åµæ¸¬å™¨ (ç´”æ¨è«–æ¨¡å¼)"""
    
    # å…è¨±åµæ¸¬çš„ COCO é¡åˆ¥ ID (å°æ‡‰ LVIS éœ€æ±‚)
    ALLOWED_CLASS_IDS = [24, 26, 39, 41, 65, 67, 73]  # backpack, handbag, bottle, cup, remote, cell phone, book
    
    # Tune æ¨¡å‹é¡åˆ¥åç¨±å°ç…§è¡¨
    TUNE_CLASS_NAMES = {
        0: "cellular phone",
        1: "remote control",
        2: "backpack",
        3: "handbag",
        4: "book",
        5: "bottle",
        6: "cup",
        7: "key",
        8: "watch",
        9: "earphone",
        10: "glasses",
        11: "notebook",
        12: "mask"
    }
    
    # Tune æ¨¡å‹é¡åˆ¥ä¸­æ–‡å°ç…§è¡¨
    TUNE_CLASS_NAMES_ZH = {
        "cellular phone": "æ‰‹æ©Ÿ",
        "remote control": "é™æ§å™¨",
        "backpack": "èƒŒåŒ…",
        "handbag": "æ‰‹æåŒ…",
        "book": "æ›¸",
        "bottle": "æ°´ç“¶",
        "cup": "æ¯å­",
        "key": "é‘°åŒ™",
        "watch": "æ‰‹éŒ¶",
        "earphone": "è€³æ©Ÿ",
        "glasses": "çœ¼é¡",
        "notebook": "ç­†è¨˜æœ¬",
        "mask": "å£ç½©"
    }
    
    # èˆ‡åŸºæœ¬ YOLO é‡ç–Šçš„ tune é¡åˆ¥ (é€™äº›é¡åˆ¥å¦‚æœå’ŒåŸºæœ¬ YOLO åµæ¸¬åˆ°åŒä¸€ç‰©ä»¶å‰‡ä¸ç¹ªè£½)
    # æ ¼å¼: tune_class_name -> å°æ‡‰çš„åŸºæœ¬ YOLO class_name
    OVERLAPPING_CLASSES = {
        "cellular phone": "cell phone",
        "remote control": "remote",
        "backpack": "backpack",
        "handbag": "handbag",
        "book": "book",
        "bottle": "bottle",
        "cup": "cup"
    }
    
    def __init__(
        self, 
        model_path: str = "yolo12l.pt",
        tune_model_path: str = "yolo12l_tune.pt",
        similarity_threshold: float = 0.7
    ):
        self.model_path = model_path
        self.tune_model_path = tune_model_path
        self.similarity_threshold = similarity_threshold
        self.model = None
        self.tune_model = None  # å¾®èª¿æ¨¡å‹
        self.feature_extractor = None
        self.object_registry = None
        self.is_ready = False
        
        # åˆå§‹åŒ–
        self._init_model()
        self._init_tune_model()
        self._init_feature_extractor()
        self._init_registry()
    
    def _init_model(self):
        """åˆå§‹åŒ– YOLO12 æ¨¡å‹"""
        try:
            import torch
            device = "cuda" if torch.cuda.is_available() else "cpu"
            if torch.cuda.is_available():
                print(f"âœ… YOLO12 æ¨¡å‹å·²è¼‰å…¥åˆ° GPU: {torch.cuda.get_device_name(0)}")
            
            self.model = YOLO(self.model_path)
            self.model.to(device)
            print(f"âœ… YOLO12 æ¨¡å‹å·²è¼‰å…¥: {self.model_path}")
            
        except Exception as e:
            print(f"âŒ YOLO12 æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            raise
    
    def _init_tune_model(self):
        """åˆå§‹åŒ–å¾®èª¿ YOLO12 æ¨¡å‹"""
        try:
            import torch
            import os
            
            # æª¢æŸ¥ tune æ¨¡å‹æª”æ¡ˆæ˜¯å¦å­˜åœ¨
            tune_path = os.path.join(os.path.dirname(__file__), self.tune_model_path)
            if not os.path.exists(tune_path):
                print(f"âš ï¸ å¾®èª¿æ¨¡å‹ä¸å­˜åœ¨: {tune_path}ï¼Œè·³éè¼‰å…¥")
                return
            
            device = "cuda" if torch.cuda.is_available() else "cpu"
            self.tune_model = YOLO(tune_path)
            self.tune_model.to(device)
            print(f"âœ… å¾®èª¿ YOLO12 æ¨¡å‹å·²è¼‰å…¥: {self.tune_model_path}")
            
        except Exception as e:
            print(f"âš ï¸ å¾®èª¿ YOLO12 æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            # ä¸æ‹‹å‡ºä¾‹å¤–ï¼Œå…è¨±ç³»çµ±åœ¨æ²’æœ‰ tune æ¨¡å‹çš„æƒ…æ³ä¸‹é‹è¡Œ
    
    def _init_feature_extractor(self):
        """åˆå§‹åŒ– DINOv2 ç‰¹å¾µæå–å™¨"""
        try:
            self.feature_extractor = FeatureExtractor()
            print("âœ… DINOv2 ç‰¹å¾µæå–å™¨å·²åˆå§‹åŒ–")
            
        except Exception as e:
            print(f"âŒ DINOv2 ç‰¹å¾µæå–å™¨åˆå§‹åŒ–å¤±æ•—: {e}")
            raise
    
    def _init_registry(self):
        """åˆå§‹åŒ–ç‰©å“è¨»å†Šè³‡æ–™åº«"""
        try:
            self.object_registry = ObjectRegistry()
            print("âœ… ç‰©å“è¨»å†Šè³‡æ–™åº«å·²è¼‰å…¥")
            self.is_ready = True
            
        except Exception as e:
            print(f"âŒ ç‰©å“è¨»å†Šè³‡æ–™åº«è¼‰å…¥å¤±æ•—: {e}")
            raise
    
    def detect_frame(self, frame: np.ndarray) -> List[DetectionResult]:
        """
        åµæ¸¬åœ–ç‰‡ä¸­çš„ç‰©å“ (ä¸»è¦æ–¹æ³•)
        åŒæ™‚ä½¿ç”¨åŸºæœ¬ YOLO å’Œå¾®èª¿æ¨¡å‹é€²è¡Œåµæ¸¬
        
        Args:
            frame: BGR æ ¼å¼çš„ numpy é™£åˆ—
            
        Returns:
            List[DetectionResult]: åµæ¸¬çµæœåˆ—è¡¨
        """
        if not self.is_ready or self.model is None:
            return []
        
        results = []
        
        try:
            # ===== 1. åŸºæœ¬ YOLO12 åµæ¸¬ =====
            yolo_results = self.model(frame, verbose=False)[0]
            
            for box in yolo_results.boxes:
                # å–å¾—åŸºæœ¬è³‡è¨Š
                cls_id = int(box.cls[0])
                conf = float(box.conf[0])
                bbox = box.xyxy[0].cpu().numpy().tolist()
                
                # ğŸ”¥ åªä¿ç•™å…è¨±çš„é¡åˆ¥
                if cls_id not in self.ALLOWED_CLASS_IDS:
                    continue
                
                # é¡åˆ¥åç¨±
                class_name = self.model.names[cls_id]
                class_name_zh = CLASS_NAMES_ZH.get(class_name, class_name)
                
                # è£åˆ‡ç‰©å“å€åŸŸ
                x1, y1, x2, y2 = [int(v) for v in bbox]
                crop = frame[y1:y2, x1:x2]
                
                if crop.size == 0:
                    continue
                
                # è¨ˆç®—ç‰©å“å€åŸŸ
                frame_height, frame_width = frame.shape[:2]
                center_x = (x1 + x2) / 2 / frame_width
                region = "å·¦å´" if center_x < 0.33 else "å³å´" if center_x > 0.67 else "ä¸­é–“"
                
                # é è¨­çµæœ
                result = DetectionResult(
                    object_class=class_name,
                    object_class_zh=class_name_zh,
                    confidence=conf,
                    bbox=bbox,
                    region=region,
                    surface="åµæ¸¬å€åŸŸ"
                )
                
                # DINOv2 ç‰¹å¾µæ¯”å°
                if len(self.object_registry.objects) > 0:
                    crop_embedding = self.feature_extractor.extract(crop)
                    
                    if crop_embedding is not None:
                        # èˆ‡å·²è¨»å†Šç‰©å“æ¯”å°
                        for obj in self.object_registry.objects.values():
                            if not obj.embeddings:
                                continue
                            
                            max_sim = 0
                            for emb in obj.embeddings:
                                sim = self.feature_extractor.cosine_similarity(crop_embedding, emb)
                                max_sim = max(max_sim, sim)
                            
                            if max_sim >= self.similarity_threshold:
                                result.matched_object_id = obj.id
                                result.matched_object_name = obj.name
                                result.matched_object_name_zh = obj.name_zh
                                result.similarity = max_sim
                                result.object_class_zh = obj.name_zh
                                break
                
                results.append(result)
            
            # ===== 2. å¾®èª¿æ¨¡å‹åµæ¸¬ (å¦‚æœæœ‰è¼‰å…¥) =====
            if self.tune_model is not None:
                tune_results = self.tune_model(frame, verbose=False)[0]
                
                for box in tune_results.boxes:
                    cls_id = int(box.cls[0])
                    conf = float(box.conf[0])
                    bbox = box.xyxy[0].cpu().numpy().tolist()
                    
                    # å–å¾— tune æ¨¡å‹çš„é¡åˆ¥åç¨±
                    tune_class_name = self.TUNE_CLASS_NAMES.get(cls_id, f"class_{cls_id}")
                    tune_class_name_zh = self.TUNE_CLASS_NAMES_ZH.get(tune_class_name, tune_class_name)
                    
                    # ğŸ”¥ å®Œå…¨è·³éé‡ç–Šé¡åˆ¥ï¼ˆä¸ç®¡ä½ç½®æ˜¯å¦é‡ç–Šï¼‰
                    if tune_class_name in self.OVERLAPPING_CLASSES:
                        continue
                    
                    # è£åˆ‡ç‰©å“å€åŸŸ
                    x1, y1, x2, y2 = [int(v) for v in bbox]
                    crop = frame[y1:y2, x1:x2]
                    
                    if crop.size == 0:
                        continue
                    
                    # è¨ˆç®—ç‰©å“å€åŸŸ
                    frame_height, frame_width = frame.shape[:2]
                    center_x = (x1 + x2) / 2 / frame_width
                    region = "å·¦å´" if center_x < 0.33 else "å³å´" if center_x > 0.67 else "ä¸­é–“"
                    
                    # é è¨­çµæœ
                    result = DetectionResult(
                        object_class=tune_class_name,
                        object_class_zh=tune_class_name_zh,
                        confidence=conf,
                        bbox=bbox,
                        region=region,
                        surface="åµæ¸¬å€åŸŸ"
                    )
                    
                    # DINOv2 ç‰¹å¾µæ¯”å°
                    if len(self.object_registry.objects) > 0:
                        crop_embedding = self.feature_extractor.extract(crop)
                        
                        if crop_embedding is not None:
                            # èˆ‡å·²è¨»å†Šç‰©å“æ¯”å°
                            for obj in self.object_registry.objects.values():
                                if not obj.embeddings:
                                    continue
                                
                                max_sim = 0
                                for emb in obj.embeddings:
                                    sim = self.feature_extractor.cosine_similarity(crop_embedding, emb)
                                    max_sim = max(max_sim, sim)
                                
                                if max_sim >= self.similarity_threshold:
                                    result.matched_object_id = obj.id
                                    result.matched_object_name = obj.name
                                    result.matched_object_name_zh = obj.name_zh
                                    result.similarity = max_sim
                                    result.object_class_zh = obj.name_zh
                                    break
                    
                    results.append(result)
                
        except Exception as e:
            print(f"âŒ åµæ¸¬éŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()
        
        return results
    
    def annotate_frame(self, frame: np.ndarray, detections: List[DetectionResult]) -> np.ndarray:
        """åœ¨åœ–ç‰‡ä¸Šç•«å‡ºåµæ¸¬æ¡†å’Œæ¨™ç±¤"""
        annotated = frame.copy()
        
        for det in detections:
            x1, y1, x2, y2 = [int(v) for v in det.bbox]
            
            # æ±ºå®šé¡è‰²ï¼ˆæœ‰åŒ¹é…åˆ°ç”¨ç¶ è‰²ï¼Œå¦å‰‡ç”¨è—è‰²ï¼‰
            if det.matched_object_id:
                color = (0, 255, 0)  # ç¶ è‰²
                # ä½¿ç”¨è‹±æ–‡åç¨±é¿å…ä¸­æ–‡æ¸²æŸ“å•é¡Œ
                label = f"{det.matched_object_name} ({det.similarity:.0%})"
            else:
                color = (255, 128, 0)  # è—è‰²
                label = f"{det.object_class} ({det.confidence:.0%})"
            
            # ç•«æ¡†
            cv2.rectangle(annotated, (x1, y1), (x2, y2), color, 2)
            
            # ç•«æ¨™ç±¤èƒŒæ™¯
            font = cv2.FONT_HERSHEY_SIMPLEX
            font_scale = 0.6
            thickness = 2
            (text_width, text_height), baseline = cv2.getTextSize(label, font, font_scale, thickness)
            
            cv2.rectangle(annotated, (x1, y1 - text_height - 10), (x1 + text_width + 10, y1), color, -1)
            cv2.putText(annotated, label, (x1 + 5, y1 - 5), font, font_scale, (255, 255, 255), thickness)
        
        return annotated
    
    # ========================================
    # ç‰©å“è¨»å†Šæ–¹æ³•
    # ========================================
    
    def register_object(self, name: str, name_zh: str, image: np.ndarray) -> Optional[dict]:
        """è¨»å†Šæ–°ç‰©å“"""
        if self.object_registry is None or self.feature_extractor is None:
            return None
        
        # ä½¿ç”¨ YOLO è£åˆ‡ä¸»è¦ç‰©ä»¶
        crops = self._extract_main_object(image)
        target_image = crops[0] if crops else image
        
        # æå–ç‰¹å¾µ
        embedding = self.feature_extractor.extract(target_image)
        if embedding is None:
            return None
        
        # å„²å­˜åœ–ç‰‡
        import os
        from datetime import datetime
        
        img_dir = os.path.join(os.path.dirname(__file__), "object_images")
        os.makedirs(img_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        img_name = f"{name}_{timestamp}.jpg"
        img_path = os.path.join(img_dir, img_name)
        cv2.imwrite(img_path, target_image)
        
        # è¨»å†Šåˆ°è³‡æ–™åº«
        obj = self.object_registry.register(
            name=name,
            name_zh=name_zh,
            embedding=embedding,
            image_path=img_path
        )
        
        if obj:
            return {
                "id": obj.id,
                "name": obj.name,
                "name_zh": obj.name_zh,
                "embedding_count": len(obj.embeddings),
                "thumbnail": f"/object_images/{img_name}"
            }
        return None
    
    def register_object_direct(self, name: str, name_zh: str, image: np.ndarray) -> Optional[dict]:
        """ç›´æ¥è¨»å†Šç‰©å“ï¼ˆä¸ä½¿ç”¨ YOLO è£åˆ‡ï¼Œé©ç”¨æ–¼å·²è£åˆ‡çš„åµæ¸¬çµæœï¼‰"""
        if self.object_registry is None or self.feature_extractor is None:
            return None
        
        # ç›´æ¥ä½¿ç”¨å‚³å…¥çš„åœ–ç‰‡ï¼Œä¸é€²è¡Œ YOLO è£åˆ‡
        embedding = self.feature_extractor.extract(image)
        if embedding is None:
            return None
        
        # å„²å­˜åœ–ç‰‡
        import os
        from datetime import datetime
        
        img_dir = os.path.join(os.path.dirname(__file__), "object_images")
        os.makedirs(img_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        img_name = f"{name}_{timestamp}.jpg"
        img_path = os.path.join(img_dir, img_name)
        cv2.imwrite(img_path, image)
        
        # è¨»å†Šåˆ°è³‡æ–™åº«
        obj = self.object_registry.register(
            name=name,
            name_zh=name_zh,
            embedding=embedding,
            image_path=img_path
        )
        
        if obj:
            return {
                "id": obj.id,
                "name": obj.name,
                "name_zh": obj.name_zh,
                "embedding_count": len(obj.embeddings),
                "thumbnail": f"/object_images/{img_name}"
            }
        return None
    
    def add_object_image(self, obj_id: str, image: np.ndarray) -> Optional[dict]:
        """ç‚ºå·²è¨»å†Šç‰©å“æ–°å¢ç…§ç‰‡"""
        if self.object_registry is None or self.feature_extractor is None:
            return None
        
        obj = self.object_registry.get(obj_id)
        if not obj:
            return None
        
        # è£åˆ‡ä¸»è¦ç‰©ä»¶
        crops = self._extract_main_object(image)
        target_image = crops[0] if crops else image
        
        # æå–ç‰¹å¾µ
        embedding = self.feature_extractor.extract(target_image)
        if embedding is None:
            return None
        
        # å„²å­˜åœ–ç‰‡
        import os
        from datetime import datetime
        
        img_dir = os.path.join(os.path.dirname(__file__), "object_images")
        os.makedirs(img_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        img_name = f"{obj.name}_{timestamp}.jpg"
        img_path = os.path.join(img_dir, img_name)
        cv2.imwrite(img_path, target_image)
        
        # æ›´æ–°ç‰©å“
        updated = self.object_registry.add_embedding(obj_id, embedding, image_path=img_path)
        
        if updated:
            return {
                "id": updated.id,
                "name": updated.name,
                "name_zh": updated.name_zh,
                "embedding_count": len(updated.embeddings)
            }
        return None
    
    def delete_object(self, obj_id: str) -> bool:
        """åˆªé™¤ç‰©å“"""
        if self.object_registry is None:
            return False
        return self.object_registry.delete(obj_id)
    
    def get_registered_objects(self) -> List[dict]:
        """å–å¾—æ‰€æœ‰å·²è¨»å†Šç‰©å“"""
        if self.object_registry is None:
            return []
        
        objects = []
        for obj in self.object_registry.objects.values():
            thumbnail = None
            if obj.images:
                import os
                thumbnail = f"/object_images/{os.path.basename(obj.images[0])}"
            
            objects.append({
                "id": obj.id,
                "name": obj.name,
                "name_zh": obj.name_zh,
                "embedding_count": len(obj.embeddings),
                "thumbnail": thumbnail
            })
        return objects
    
    def _extract_main_object(self, image: np.ndarray) -> List[np.ndarray]:
        """ä½¿ç”¨ YOLO è£åˆ‡åœ–ç‰‡ä¸­çš„ä¸»è¦ç‰©ä»¶"""
        if self.model is None:
            return []
        
        try:
            results = self.model(image, verbose=False)[0]
            crops = []
            
            for box in results.boxes:
                conf = float(box.conf[0])
                if conf < 0.3:
                    continue
                
                x1, y1, x2, y2 = [int(v) for v in box.xyxy[0].cpu().numpy()]
                
                # æ“´å±•é‚Šç•Œ
                h, w = image.shape[:2]
                pad = 10
                x1 = max(0, x1 - pad)
                y1 = max(0, y1 - pad)
                x2 = min(w, x2 + pad)
                y2 = min(h, y2 + pad)
                
                crop = image[y1:y2, x1:x2]
                if crop.size > 0:
                    crops.append(crop)
            
            # æŒ‰é¢ç©æ’åºï¼Œè¿”å›æœ€å¤§çš„
            crops.sort(key=lambda c: c.shape[0] * c.shape[1], reverse=True)
            return crops[:1] if crops else []
            
        except Exception as e:
            print(f"âš ï¸ ç‰©ä»¶è£åˆ‡å¤±æ•—: {e}")
            return []
